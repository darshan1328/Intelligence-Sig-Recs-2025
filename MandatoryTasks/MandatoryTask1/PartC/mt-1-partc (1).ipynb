{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31153,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Brief Overview:**# \n\nI have followed the below steps to fine-tune my distil-bert model with the NewsQA dataset\n\n1. Loading and exploring the dataset.\n\n2. Initializing the BERT model and tokenizer.\n\n3. Preprocessing the dataset to prepare inputs for the model.\n\n4. Configuring lightweight training parameters for efficient fine-tuning.\n\n5. Training the model on the dataset and saving the fine-tuned version.\n\n6. Loading the trained model and predicting answers for given questions and contexts.\n\n7. Providing a practical example of using the fine-tuned model for real-world question-answering.","metadata":{}},{"cell_type":"code","source":"pip install -q transformers datasets","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-22T03:55:18.065862Z","iopub.execute_input":"2025-10-22T03:55:18.066128Z","iopub.status.idle":"2025-10-22T03:55:28.524336Z","shell.execute_reply.started":"2025-10-22T03:55:18.066107Z","shell.execute_reply":"2025-10-22T03:55:28.523239Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m564.3/564.3 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.8/42.8 MB\u001b[0m \u001b[31m37.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 2.12.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\npylibcudf-cu12 25.2.2 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 21.0.0 which is incompatible.\ncudf-cu12 25.2.2 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 21.0.0 which is incompatible.\nbigframes 2.12.0 requires google-cloud-bigquery[bqstorage,pandas]>=3.31.0, but you have google-cloud-bigquery 3.25.0 which is incompatible.\nbigframes 2.12.0 requires rich<14,>=12.4.4, but you have rich 14.1.0 which is incompatible.\ngradio 5.38.1 requires pydantic<2.12,>=2.0, but you have pydantic 2.12.0a1 which is incompatible.\ncudf-polars-cu12 25.6.0 requires pylibcudf-cu12==25.6.*, but you have pylibcudf-cu12 25.2.2 which is incompatible.\npandas-gbq 0.29.2 requires google-api-core<3.0.0,>=2.10.2, but you have google-api-core 1.34.1 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"from transformers import (\n    DistilBertTokenizerFast,\n    DistilBertForQuestionAnswering,\n    TrainingArguments,\n    Trainer,\n    DataCollatorWithPadding  \n)\nimport torch","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-22T04:01:32.829708Z","iopub.execute_input":"2025-10-22T04:01:32.830325Z","iopub.status.idle":"2025-10-22T04:02:04.365350Z","shell.execute_reply.started":"2025-10-22T04:01:32.830298Z","shell.execute_reply":"2025-10-22T04:02:04.364609Z"}},"outputs":[{"name":"stderr","text":"2025-10-22 04:01:47.961050: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1761105708.186430      37 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1761105708.253318      37 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":4},{"cell_type":"markdown","source":"## **Load the Model and Tokenizer**","metadata":{}},{"cell_type":"code","source":"model_name = \"distilbert/distilbert-base-uncased\" \n\ntokenizer = DistilBertTokenizerFast.from_pretrained(model_name)\nmodel = DistilBertForQuestionAnswering.from_pretrained(model_name)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-22T04:02:15.819115Z","iopub.execute_input":"2025-10-22T04:02:15.819857Z","iopub.status.idle":"2025-10-22T04:02:19.305524Z","shell.execute_reply.started":"2025-10-22T04:02:15.819831Z","shell.execute_reply":"2025-10-22T04:02:19.304763Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e1027cb0f4254b169a3ca40aa22bc271"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7613f2ddfc444927b4c468ac950d3ad2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3c9213762a9448db8a1101fa934ff347"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1c4caccf181e485e8250ba1aa6da0146"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d174525c29844903bb77b8bd40bbf8aa"}},"metadata":{}},{"name":"stderr","text":"Some weights of DistilBertForQuestionAnswering were not initialized from the model checkpoint at distilbert/distilbert-base-uncased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}],"execution_count":5},{"cell_type":"markdown","source":"## **Loading the NewsQA dataset** ","metadata":{}},{"cell_type":"code","source":"from datasets import load_dataset\n\nds_all = load_dataset(\"lucadiliello/newsqa\")\neval_name = \"validation\" if \"validation\" in ds_all else (\"dev\" if \"dev\" in ds_all else \"test\")\n\ntrain_ds = load_dataset(\"lucadiliello/newsqa\", split=\"train[:200]\")\neval_ds  = load_dataset(\"lucadiliello/newsqa\", split=f\"{eval_name}[:40]\")\n\nprint(train_ds)\nprint(eval_ds)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-22T04:09:26.493593Z","iopub.execute_input":"2025-10-22T04:09:26.493985Z","iopub.status.idle":"2025-10-22T04:09:29.260154Z","shell.execute_reply.started":"2025-10-22T04:09:26.493960Z","shell.execute_reply":"2025-10-22T04:09:29.259229Z"}},"outputs":[{"name":"stdout","text":"Dataset({\n    features: ['context', 'question', 'answers', 'key', 'labels'],\n    num_rows: 200\n})\nDataset({\n    features: ['context', 'question', 'answers', 'key', 'labels'],\n    num_rows: 40\n})\n","output_type":"stream"}],"execution_count":6},{"cell_type":"markdown","source":"## **Preprocessing**","metadata":{}},{"cell_type":"code","source":"def _first_int(x):\n    \n    if isinstance(x, (int, float)):\n        return int(x)\n    if isinstance(x, (list, tuple)):\n        for v in x:\n            if isinstance(v, (int, float)):\n                return int(v)\n            try:\n                return int(v)\n            except Exception:\n                continue\n        return None\n    try:\n        return int(x)\n    except Exception:\n        return None\n\ndef preprocess_newsqa(examples):\n    questions = [q.strip() for q in examples[\"question\"]]\n    contexts  = [c.strip() for c in examples[\"context\"]]\n\n    enc = tokenizer(\n        questions,\n        contexts,\n        max_length=256,\n        truncation=\"only_second\",\n        stride=128,\n        return_offsets_mapping=True,\n        padding=\"max_length\",\n    )\n\n    start_positions, end_positions = [], []\n\n    for i, offsets in enumerate(enc[\"offset_mapping\"]):\n        ctx = contexts[i]\n        start_char, end_char = None, None\n\n        \n        if \"labels\" in examples:\n            labs = examples[\"labels\"][i]\n            if labs and isinstance(labs, list):\n                lab = labs[0]  # pick first label\n                if isinstance(lab, dict):\n                    s = _first_int(lab.get(\"start\"))\n                    e = _first_int(lab.get(\"end\"))\n                    ln = _first_int(lab.get(\"len\"))\n                    if s is not None and (e is not None or ln is not None):\n                        start_char = s\n                        end_char = e if e is not None else (s + ln if ln is not None else None)\n\n        \n        if (start_char is None or end_char is None) and \"answers\" in examples:\n            ans_list = examples[\"answers\"][i]\n            if isinstance(ans_list, list) and len(ans_list) > 0 and isinstance(ans_list[0], str):\n                ans_text = ans_list[0]\n                idx = ctx.lower().find(ans_text.lower())\n                if idx != -1:\n                    start_char = idx\n                    end_char = idx + len(ans_text)\n\n        \n        input_ids = enc[\"input_ids\"][i]\n        cls_index = input_ids.index(tokenizer.cls_token_id) if tokenizer.cls_token_id in input_ids else 0\n        if start_char is None or end_char is None:\n            start_positions.append(cls_index)\n            end_positions.append(cls_index)\n            continue\n\n        start_tok, end_tok = None, None\n        for t, (s, e) in enumerate(offsets):\n            if s <= start_char <= e:\n                start_tok = t\n            if s <= end_char <= e:\n                end_tok = t\n                break\n\n        if start_tok is None or end_tok is None or end_tok < start_tok:\n            start_positions.append(cls_index)\n            end_positions.append(cls_index)\n        else:\n            start_positions.append(start_tok)\n            end_positions.append(end_tok)\n\n    enc[\"start_positions\"] = start_positions\n    enc[\"end_positions\"] = end_positions\n    enc.pop(\"offset_mapping\")\n    return enc\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-22T04:20:43.313103Z","iopub.execute_input":"2025-10-22T04:20:43.314115Z","iopub.status.idle":"2025-10-22T04:20:43.335847Z","shell.execute_reply.started":"2025-10-22T04:20:43.314073Z","shell.execute_reply":"2025-10-22T04:20:43.334708Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"tokenized_train = train_ds.map(\n    preprocess_newsqa,\n    batched=True,\n    batch_size=32,\n    remove_columns=train_ds.column_names,\n)\n\ntokenized_eval = eval_ds.map(\n    preprocess_newsqa,\n    batched=True,\n    batch_size=32,\n    remove_columns=eval_ds.column_names,\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-22T04:21:23.852628Z","iopub.execute_input":"2025-10-22T04:21:23.853003Z","iopub.status.idle":"2025-10-22T04:21:24.358294Z","shell.execute_reply.started":"2025-10-22T04:21:23.852980Z","shell.execute_reply":"2025-10-22T04:21:24.357051Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/200 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"117df11729a74920be4f97c010a1e9d0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/40 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"297a3d8e8c554dc6a71b31aac673f7fc"}},"metadata":{}}],"execution_count":13},{"cell_type":"code","source":"pip -q install transformers[torch]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-22T04:21:47.487771Z","iopub.execute_input":"2025-10-22T04:21:47.488717Z","iopub.status.idle":"2025-10-22T04:23:21.548869Z","shell.execute_reply.started":"2025-10-22T04:21:47.488658Z","shell.execute_reply":"2025-10-22T04:23:21.547382Z"}},"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m81.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m29.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m31.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m28.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m73.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nlibcugraph-cu12 25.6.0 requires libraft-cu12==25.6.*, but you have libraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 25.6.0 requires pylibraft-cu12==25.6.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 25.6.0 requires rmm-cu12==25.6.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":14},{"cell_type":"markdown","source":"## **Configuring Training Parameters for Fast and Lightweight Model Training**","metadata":{}},{"cell_type":"code","source":"from transformers import TrainingArguments\nimport transformers as tfm\nfrom packaging import version\n\neval_key = \"eval_strategy\" if version.parse(tfm.__version__) >= version.parse(\"4.46\") else \"evaluation_strategy\"\n\nUSE_CPU = False \n\ntraining_args = TrainingArguments(\n    output_dir=\"./newsqa-distilbert-results\",\n    num_train_epochs=1,\n    per_device_train_batch_size=4,\n    per_device_eval_batch_size=4,\n    learning_rate=5e-5,\n    weight_decay=0.01,\n    logging_steps=10,\n    save_strategy=\"no\",\n    report_to=\"none\",\n    no_cuda=USE_CPU,            \n    **{eval_key: \"no\"}          \n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-22T04:25:02.995301Z","iopub.execute_input":"2025-10-22T04:25:02.995894Z","iopub.status.idle":"2025-10-22T04:25:03.090458Z","shell.execute_reply.started":"2025-10-22T04:25:02.995844Z","shell.execute_reply":"2025-10-22T04:25:03.089192Z"}},"outputs":[],"execution_count":15},{"cell_type":"markdown","source":"## **Initalize and Train**","metadata":{}},{"cell_type":"code","source":"from transformers import Trainer, DataCollatorWithPadding\n\ndata_collator = DataCollatorWithPadding(tokenizer)\n\ntrainer = Trainer(\n    model=model,                         \n    args=training_args,\n    train_dataset=tokenized_train,      \n    eval_dataset=tokenized_eval,        \n    data_collator=data_collator\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-22T04:25:22.759926Z","iopub.execute_input":"2025-10-22T04:25:22.760252Z","iopub.status.idle":"2025-10-22T04:25:22.807503Z","shell.execute_reply.started":"2025-10-22T04:25:22.760231Z","shell.execute_reply":"2025-10-22T04:25:22.806769Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"trainer.train()  \n\ntrainer.save_model(\"./newsqa-distilbert-model\") \n\nmodel.save_pretrained(\"./newsqa-distilbert-model\")          \ntokenizer.save_pretrained(\"./newsqa-distilbert-model\")    ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-22T04:26:13.423629Z","iopub.execute_input":"2025-10-22T04:26:13.424010Z","iopub.status.idle":"2025-10-22T04:28:49.984447Z","shell.execute_reply.started":"2025-10-22T04:26:13.423985Z","shell.execute_reply":"2025-10-22T04:28:49.983626Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='50' max='50' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [50/50 02:28, Epoch 1/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>10</td>\n      <td>5.404700</td>\n    </tr>\n    <tr>\n      <td>20</td>\n      <td>5.158200</td>\n    </tr>\n    <tr>\n      <td>30</td>\n      <td>4.882100</td>\n    </tr>\n    <tr>\n      <td>40</td>\n      <td>4.685300</td>\n    </tr>\n    <tr>\n      <td>50</td>\n      <td>4.433400</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"('./newsqa-distilbert-model/tokenizer_config.json',\n './newsqa-distilbert-model/special_tokens_map.json',\n './newsqa-distilbert-model/vocab.txt',\n './newsqa-distilbert-model/added_tokens.json',\n './newsqa-distilbert-model/tokenizer.json')"},"metadata":{}}],"execution_count":17},{"cell_type":"markdown","source":"## **Example usage**","metadata":{}},{"cell_type":"code","source":"from transformers import pipeline\n\nqa = pipeline(\n    \"question-answering\",\n    model=\"./newsqa-distilbert-model\",\n    tokenizer=\"./newsqa-distilbert-model\",\n)\n\ncontext = \"NEW DELHI, India (CNN) -- A high court in northern India...\"\nquestion = \"When was Pandher sentenced to death?\"\nprint(qa(question=question, context=context))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-22T04:31:49.965989Z","iopub.execute_input":"2025-10-22T04:31:49.966363Z","iopub.status.idle":"2025-10-22T04:31:50.778585Z","shell.execute_reply.started":"2025-10-22T04:31:49.966340Z","shell.execute_reply":"2025-10-22T04:31:50.777714Z"}},"outputs":[{"name":"stderr","text":"Device set to use cpu\n","output_type":"stream"},{"name":"stdout","text":"{'score': 0.00982893817126751, 'start': 11, 'end': 50, 'answer': 'India (CNN) -- A high court in northern'}\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"from transformers import pipeline\nqa = pipeline(\n    \"question-answering\",\n    model=\"./newsqa-distilbert-model\",\n    tokenizer=\"./newsqa-distilbert-model\",\n    max_seq_len=256,\n    doc_stride=128,\n    handle_impossible_answer=False,\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-22T04:34:46.384650Z","iopub.execute_input":"2025-10-22T04:34:46.385131Z","iopub.status.idle":"2025-10-22T04:34:46.489749Z","shell.execute_reply.started":"2025-10-22T04:34:46.385101Z","shell.execute_reply":"2025-10-22T04:34:46.488487Z"}},"outputs":[{"name":"stderr","text":"Device set to use cpu\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"pred_texts = []\ngold_texts_list = []\n\nfor ex in eval_ds:\n    pr = qa(question=ex[\"question\"], context=ex[\"context\"])\n    pred_texts.append(pr[\"answer\"])\n    golds = ex[\"answers\"] if isinstance(ex.get(\"answers\"), list) else []\n    gold_texts_list.append([g for g in dict.fromkeys(golds) if isinstance(g, str) and g.strip()])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-22T04:34:54.719929Z","iopub.execute_input":"2025-10-22T04:34:54.720221Z","iopub.status.idle":"2025-10-22T04:35:41.620704Z","shell.execute_reply.started":"2025-10-22T04:34:54.720203Z","shell.execute_reply":"2025-10-22T04:35:41.619643Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"import re, string\ndef normalize_text(s):\n    def remove_articles(text): return re.sub(r\"\\b(a|an|the)\\b\", \" \", text)\n    def white_space_fix(text):  return \" \".join(text.split())\n    def remove_punc(text):      return text.translate(str.maketrans(\"\", \"\", string.punctuation))\n    def lower(text):            return text.lower()\n    return white_space_fix(remove_articles(remove_punc(lower(s))))\n\ndef f1_single(prediction, ground_truth):\n    pred_tokens = normalize_text(prediction).split()\n    gold_tokens = normalize_text(ground_truth).split()\n    if len(pred_tokens) == 0 and len(gold_tokens) == 0:\n        return 1.0\n    if len(pred_tokens) == 0 or len(gold_tokens) == 0:\n        return 0.0\n    common = {}\n    for t in pred_tokens:\n        common[t] = common.get(t, 0) + (t in gold_tokens)\n    num_same = sum(min(pred_tokens.count(t), gold_tokens.count(t)) for t in set(pred_tokens))\n    if num_same == 0:\n        return 0.0\n    precision = num_same / len(pred_tokens)\n    recall    = num_same / len(gold_tokens)\n    return 2 * precision * recall / (precision + recall)\n\ndef exact_match_single(prediction, ground_truth):\n    return 1.0 if normalize_text(prediction) == normalize_text(ground_truth) else 0.0\n\ndef best_over_gold(metric_fn, prediction, gold_list):\n    if not gold_list:\n        return metric_fn(prediction, \"\") \n    return max(metric_fn(prediction, g) for g in gold_list)\n\nem_scores, f1_scores = [], []\nfor pred, golds in zip(pred_texts, gold_texts_list):\n    em_scores.append(best_over_gold(exact_match_single, pred, golds))\n    f1_scores.append(best_over_gold(f1_single, pred, golds))\n\nem = sum(em_scores) / len(em_scores) if em_scores else 0.0\nf1 = sum(f1_scores) / len(f1_scores) if f1_scores else 0.0\nprint({\"EM\": em, \"F1\": f1})\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-22T04:36:09.916412Z","iopub.execute_input":"2025-10-22T04:36:09.916798Z","iopub.status.idle":"2025-10-22T04:36:09.930878Z","shell.execute_reply.started":"2025-10-22T04:36:09.916770Z","shell.execute_reply":"2025-10-22T04:36:09.929824Z"}},"outputs":[{"name":"stdout","text":"{'EM': 0.075, 'F1': 0.13916666666666666}\n","output_type":"stream"}],"execution_count":21},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}